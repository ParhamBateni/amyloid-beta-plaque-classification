{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test PlaqueDataset vs Notebook Dataset\n",
        "\n",
        "This notebook tests whether the training performance difference is due to:\n",
        "1. Dataset differences (PlaqueDataset vs notebook's custom dataset)\n",
        "2. Model architecture differences\n",
        "\n",
        "We'll use the same model architecture as the notebook but with your PlaqueDataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "from typing import Optional, Any, Tuple\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    accuracy_score\n",
        ")\n",
        "\n",
        "# Import your PlaqueDataset\n",
        "import sys\n",
        "sys.path.append('src')\n",
        "from models.data.plaque_dataset import PlaqueDataset, load_dataloaders\n",
        "from models.config import Config\n",
        "from utils import load_config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Set parameters to match notebook\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "NUM_WORKERS = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preloading labeled plaque images...: 100%|██████████| 376/376 [00:02<00:00, 127.10it/s]\n",
            "Preloading test labeled plaque images...: 100%|██████████| 118/118 [00:01<00:00, 100.73it/s]\n",
            "Preloading val labeled plaque images...: 100%|██████████| 95/95 [00:00<00:00, 111.46it/s]\n",
            "Preloading unlabeled plaque images...: 100%|██████████| 2000/2000 [00:20<00:00, 95.74it/s] \n",
            "Train loader batches: 12\n",
            "Val loader batches: 3\n",
            "Test loader batches: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\DELL\\Desktop\\Honours\\pbt-plaque-analysis\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample batch structure:\n",
            "Number of items in batch: 7\n",
            "Item 0: <class 'tuple'> - ('data\\\\labeled_images\\\\Diffuse\\\\Image_2018-086_lobparinf_AB4_index_3978.png', 'data\\\\labeled_images\\\\Coarse\\\\Image_2014-049_Occipital_AB_index_2336.png', 'data\\\\labeled_images\\\\Coarse\\\\Image_2021-075_occipitalA_AB4_index_7004.png')\n",
            "Item 1: shape torch.Size([32, 3, 224, 224]), dtype torch.float32\n",
            "Item 2: shape torch.Size([32, 3, 224, 224]), dtype torch.float32\n",
            "Item 3: shape torch.Size([32, 3, 224, 224]), dtype torch.float32\n",
            "Item 4: shape torch.Size([32, 3, 224, 224]), dtype torch.float32\n",
            "Item 5: shape torch.Size([32, 0]), dtype torch.float32\n",
            "Item 6: shape torch.Size([32]), dtype torch.int64\n"
          ]
        }
      ],
      "source": [
        "# Load your PlaqueDataset using the same config as supervised_model.py\n",
        "config = load_config(\"configs\", \"supervised\")\n",
        "train_loader, val_loader, test_loader, unlabeled_loader = load_dataloaders(config)\n",
        "\n",
        "print(f\"Train loader batches: {len(train_loader)}\")\n",
        "print(f\"Val loader batches: {len(val_loader)}\")\n",
        "print(f\"Test loader batches: {len(test_loader)}\")\n",
        "\n",
        "# Get a sample batch to check data format\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample batch structure:\")\n",
        "print(f\"Number of items in batch: {len(sample_batch)}\")\n",
        "for i, item in enumerate(sample_batch):\n",
        "    if hasattr(item, 'shape'):\n",
        "        print(f\"Item {i}: shape {item.shape}, dtype {item.dtype}\")\n",
        "    else:\n",
        "        print(f\"Item {i}: {type(item)} - {item[:3] if len(item) > 3 else item}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with 9 classes\n",
            "Total parameters: 25,785,929\n"
          ]
        }
      ],
      "source": [
        "# Define the same SimpleCNN model as in the notebook\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 28 * 28, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "num_classes = len(config.name_to_label)\n",
        "simple_model = SimpleCNN(num_classes=num_classes).to(device)\n",
        "print(f\"Model created with {num_classes} classes\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in simple_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes:\n",
            "  scaled_normalized_transformed_images: torch.Size([32, 3, 224, 224])\n",
            "  labels: torch.Size([32])\n",
            "  Input range: [0.00, 255.00]\n",
            "\n",
            "Model test:\n",
            "  Input shape: torch.Size([4, 3, 224, 224])\n",
            "  Output shape: torch.Size([4, 9])\n",
            "  Output range: [-3.64, 7.22]\n",
            "  Predictions: [3 3 3 3]\n",
            "  Unique predictions: [3]\n"
          ]
        }
      ],
      "source": [
        "# Test model with PlaqueDataset data\n",
        "sample_batch = next(iter(train_loader))\n",
        "(image_paths, scaled_raw_images, scaled_transformed_images, \n",
        " scaled_normalized_raw_images, scaled_normalized_transformed_images, \n",
        " extra_features, labels) = sample_batch\n",
        "\n",
        "print(f\"Input shapes:\")\n",
        "print(f\"  scaled_normalized_transformed_images: {scaled_normalized_transformed_images.shape}\")\n",
        "print(f\"  labels: {labels.shape}\")\n",
        "print(f\"  Input range: [{scaled_normalized_transformed_images.min():.2f}, {scaled_normalized_transformed_images.max():.2f}]\")\n",
        "\n",
        "# Test forward pass\n",
        "simple_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_input = scaled_normalized_transformed_images[:4].to(device)\n",
        "    test_output = simple_model(test_input)\n",
        "    print(f\"\\nModel test:\")\n",
        "    print(f\"  Input shape: {test_input.shape}\")\n",
        "    print(f\"  Output shape: {test_output.shape}\")\n",
        "    print(f\"  Output range: [{test_output.min():.2f}, {test_output.max():.2f}]\")\n",
        "    \n",
        "    # Check predictions\n",
        "    predictions = torch.argmax(test_output, dim=1)\n",
        "    print(f\"  Predictions: {predictions.cpu().numpy()}\")\n",
        "    print(f\"  Unique predictions: {torch.unique(predictions).cpu().numpy()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function (same as notebook)\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    val_loader: Optional[DataLoader] = None,\n",
        "    device: str = 'cpu',\n",
        "    num_epochs: int = 10,\n",
        "    early_stopping: bool = True,\n",
        "    patience: int = 5\n",
        "):\n",
        "    model.train()\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        # Training phase\n",
        "        model.train()\n",
        "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "            # Extract data from PlaqueDataset batch format\n",
        "            (image_paths, scaled_raw_images, scaled_transformed_images, \n",
        "             scaled_normalized_raw_images, scaled_normalized_transformed_images, \n",
        "             extra_features, labels) = batch\n",
        "            \n",
        "            # Use the same input as your supervised_model.py\n",
        "            images = scaled_normalized_transformed_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            # Print progress for first few batches\n",
        "            if batch_idx < 3:\n",
        "                print(f\"Batch {batch_idx}: Loss={loss.item():.4f}, Acc={100.*correct/total:.2f}%\")\n",
        "                print(f\"  Predictions: {predicted[:5].cpu().numpy()}\")\n",
        "                print(f\"  Labels: {labels[:5].cpu().numpy()}\")\n",
        "        \n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = 100. * correct / total\n",
        "        \n",
        "        # Validation phase\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    (image_paths, scaled_raw_images, scaled_transformed_images, \n",
        "                     scaled_normalized_raw_images, scaled_normalized_transformed_images, \n",
        "                     extra_features, labels) = batch\n",
        "                    \n",
        "                    images = scaled_normalized_transformed_images.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    \n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    val_loss += loss.item() * images.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "        else:\n",
        "            val_loss = 0.0\n",
        "            val_acc = 0.0\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Train Acc: {epoch_acc:.2f}% | Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.2f}%\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if early_stopping and val_loader is not None:\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING WITH PLAQUEDATASET\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:   8%|▊         | 1/12 [00:03<00:43,  3.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Loss=5.0498, Acc=12.50%\n",
            "  Predictions: [3 3 7 5 3]\n",
            "  Labels: [0 3 3 3 5]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  17%|█▋        | 2/12 [00:07<00:39,  3.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1: Loss=7.7237, Acc=7.81%\n",
            "  Predictions: [0 2 1 4 2]\n",
            "  Labels: [1 7 2 0 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5:  25%|██▌       | 3/12 [00:11<00:33,  3.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 2: Loss=13.6345, Acc=10.42%\n",
            "  Predictions: [1 1 1 1 1]\n",
            "  Labels: [2 0 2 8 8]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 12/12 [00:40<00:00,  3.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Train Loss: 8.1515 - Train Acc: 12.23% | Val Loss: 3.2547 - Val Acc: 12.63%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:   8%|▊         | 1/12 [00:03<00:41,  3.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Loss=3.9271, Acc=12.50%\n",
            "  Predictions: [8 2 1 0 0]\n",
            "  Labels: [0 3 3 3 5]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  17%|█▋        | 2/12 [00:07<00:34,  3.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1: Loss=3.2072, Acc=15.62%\n",
            "  Predictions: [5 1 1 0 1]\n",
            "  Labels: [1 7 2 0 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  25%|██▌       | 3/12 [00:10<00:30,  3.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 2: Loss=3.0462, Acc=14.58%\n",
            "  Predictions: [2 2 1 2 0]\n",
            "  Labels: [2 0 2 8 8]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 12/12 [00:34<00:00,  2.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 - Train Loss: 3.0491 - Train Acc: 15.16% | Val Loss: 2.4621 - Val Acc: 13.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:   8%|▊         | 1/12 [00:02<00:29,  2.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Loss=2.5170, Acc=18.75%\n",
            "  Predictions: [0 0 1 1 0]\n",
            "  Labels: [0 3 3 3 5]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  17%|█▋        | 2/12 [00:05<00:26,  2.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1: Loss=2.1504, Acc=20.31%\n",
            "  Predictions: [1 1 1 0 0]\n",
            "  Labels: [1 7 2 0 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5:  25%|██▌       | 3/12 [00:08<00:24,  2.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 2: Loss=2.4587, Acc=17.71%\n",
            "  Predictions: [1 0 1 1 1]\n",
            "  Labels: [2 0 2 8 8]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 12/12 [00:32<00:00,  2.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 - Train Loss: 2.2788 - Train Acc: 17.82% | Val Loss: 2.1389 - Val Acc: 15.79%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:   8%|▊         | 1/12 [00:02<00:29,  2.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Loss=2.1289, Acc=18.75%\n",
            "  Predictions: [5 1 1 1 1]\n",
            "  Labels: [0 3 3 3 5]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  17%|█▋        | 2/12 [00:05<00:26,  2.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1: Loss=2.1125, Acc=20.31%\n",
            "  Predictions: [0 0 1 0 7]\n",
            "  Labels: [1 7 2 0 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5:  25%|██▌       | 3/12 [00:07<00:23,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 2: Loss=2.2293, Acc=17.71%\n",
            "  Predictions: [0 7 1 1 0]\n",
            "  Labels: [2 0 2 8 8]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 12/12 [00:32<00:00,  2.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 - Train Loss: 2.1580 - Train Acc: 16.76% | Val Loss: 2.1448 - Val Acc: 16.84%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5:   8%|▊         | 1/12 [00:02<00:30,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Loss=2.1755, Acc=15.62%\n",
            "  Predictions: [1 1 1 5 0]\n",
            "  Labels: [0 3 3 3 5]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5:  17%|█▋        | 2/12 [00:05<00:27,  2.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1: Loss=2.0994, Acc=17.19%\n",
            "  Predictions: [0 1 1 1 1]\n",
            "  Labels: [1 7 2 0 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5:  25%|██▌       | 3/12 [00:08<00:25,  2.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 2: Loss=2.1683, Acc=18.75%\n",
            "  Predictions: [2 0 5 1 5]\n",
            "  Labels: [2 0 2 8 8]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 12/12 [00:33<00:00,  2.78s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 - Train Loss: 2.1370 - Train Acc: 15.96% | Val Loss: 2.1178 - Val Acc: 16.84%\n"
          ]
        }
      ],
      "source": [
        "# Train the model with PlaqueDataset\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING WITH PLAQUEDATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(simple_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Train for 5 epochs to compare with notebook\n",
        "train(\n",
        "    simple_model, \n",
        "    train_loader, \n",
        "    criterion, \n",
        "    optimizer, \n",
        "    val_loader=val_loader, \n",
        "    device=device, \n",
        "    num_epochs=5,  # Same as notebook comparison\n",
        "    early_stopping=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with notebook results\n",
        "print(\"=\"*60)\n",
        "print(\"COMPARISON WITH NOTEBOOK RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"Notebook results (5 epochs):\")\n",
        "print(\"  Epoch 1: Train Acc: 15.43%\")\n",
        "print(\"  Epoch 2: Train Acc: 24.73%\")\n",
        "print(\"  Epoch 3: Train Acc: 35.11%\")\n",
        "print(\"  Epoch 4: Train Acc: 40.16%\")\n",
        "print(\"  Epoch 5: Train Acc: 45.74%\")\n",
        "print(\"\\nYour PlaqueDataset results (5 epochs):\")\n",
        "print(\"  [Results will be shown above after training]\")\n",
        "print(\"\\nAnalysis:\")\n",
        "print(\"  - If PlaqueDataset shows similar progress → Model architecture issue\")\n",
        "print(\"  - If PlaqueDataset shows different progress → Dataset issue\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
